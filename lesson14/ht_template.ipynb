{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import coalesce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 17:23:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Домашнє завдання на тему Spark SQL\n",
    "\n",
    "Задачі з домашнього завдання на SQL потрібно розвʼязати за допомогою Spark SQL DataFrame API.\n",
    "\n",
    "Дампи таблиць знаходяться в папці `data`.\n",
    "Можете створювати стільки нових клітинок, скільки вам необхідно.\n",
    "\n",
    "Розвʼязок кожної задачі має бути відображений в самому файлі (використати метод `.show()`)\n",
    "\n",
    "**Увага!** Використовувати мову запитів SQL безпосередньо забороняється, потрібно використовувати виключно DataFrame API!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1. Вивести кількість фільмів в кожній категорії. Результат відсортувати за спаданням."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|category_id|count|\n",
      "+-----------+-----+\n",
      "|         15|   74|\n",
      "|          9|   73|\n",
      "|          8|   69|\n",
      "|          6|   68|\n",
      "|          2|   66|\n",
      "|          1|   64|\n",
      "|         13|   63|\n",
      "|          7|   62|\n",
      "|         10|   61|\n",
      "|         14|   61|\n",
      "|          3|   60|\n",
      "|          5|   58|\n",
      "|         16|   57|\n",
      "|          4|   57|\n",
      "|         11|   56|\n",
      "|         12|   51|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header='True', inferSchema='True').csv(\"data/film_category.csv\")\n",
    "df.groupBy('category_id').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2. Вивести 10 акторів, чиї фільми брали на прокат найбільше. Результат відсортувати за спаданням."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|actor_id|\n",
      "+--------+\n",
      "|     195|\n",
      "|     194|\n",
      "|     193|\n",
      "|      92|\n",
      "|      89|\n",
      "|      51|\n",
      "|      42|\n",
      "|      35|\n",
      "|      32|\n",
      "|      26|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actor_film = spark.read.options(header='True', inferSchema='True').csv(\"data/film_actor.csv\")\n",
    "rental = spark.read.options(header='True', inferSchema='True').csv(\"data/rental.csv\")\n",
    "inventory = spark.read.options(header='True', inferSchema='True').csv(\"data/inventory.csv\")\n",
    "\n",
    "top_movie = rental.join(inventory, on='inventory_id').groupBy('film_id').count()\n",
    "actor_film.join(top_movie, on='film_id')\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .select('actor_id')\\\n",
    "    .limit(10)\\\n",
    "    .sort('actor_id', ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3. Вивести категорія фільмів, на яку було витрачено найбільше грошей в прокаті"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Sports|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental = spark.read.options(header='True', inferSchema='True').csv(\"data/rental.csv\")\n",
    "inventory = spark.read.options(header='True', inferSchema='True').csv(\"data/inventory.csv\")\n",
    "film_category = spark.read.options(header='True', inferSchema='True').csv(\"data/film_category.csv\")\n",
    "categories = spark.read.options(header='True', inferSchema='True').csv(\"data/category.csv\")\n",
    "payment = spark.read.options(header='True', inferSchema='True').csv(\"data/payment.csv\")\n",
    "\n",
    "sales = rental.join(payment, on='rental_id')\\\n",
    "    .groupBy('inventory_id')\\\n",
    "    .sum('amount')\\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"income\")\n",
    "\n",
    "sales_by_film_category = sales.join(inventory, on='inventory_id')\\\n",
    "    .join(film_category, on='film_id')\\\n",
    "    .groupby('category_id')\\\n",
    "    .sum('income')\\\n",
    "    .withColumnRenamed(\"sum(income)\", \"income\")\n",
    "\n",
    "sales_by_film_category.join(categories, on='category_id')\\\n",
    "    .sort('income', ascending=False)\\\n",
    "    .select('name')\\\n",
    "    .limit(1)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 4. Вивести назви фільмів, яких немає в inventory. Запит має бути без оператора IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|      ALICE FANTASIA|\n",
      "|         APOLLO TEEN|\n",
      "|      ARGONAUTS TOWN|\n",
      "|       ARK RIDGEMONT|\n",
      "|ARSENIC INDEPENDENCE|\n",
      "|   BOONDOCK BALLROOM|\n",
      "|       BUTCH PANTHER|\n",
      "|       CATCH AMISTAD|\n",
      "| CHINATOWN GLADIATOR|\n",
      "|      CHOCOLATE DUCK|\n",
      "|COMMANDMENTS EXPRESS|\n",
      "|    CROSSING DIVORCE|\n",
      "|     CROWDS TELEMARK|\n",
      "|    CRYSTAL BREAKING|\n",
      "|          DAZED PUNK|\n",
      "|DELIVERANCE MULHO...|\n",
      "|   FIREHOUSE VIETNAM|\n",
      "|       FLOATS GARDEN|\n",
      "|FRANKENSTEIN STRA...|\n",
      "|  GLADIATOR WESTWARD|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "film = spark.read.options(header='True', inferSchema='True').csv(\"data/film.csv\")\n",
    "inventory = spark.read.options(header='True', inferSchema='True').csv(\"data/inventory.csv\")\n",
    "\n",
    "film.join(inventory, on='film_id', how='left')\\\n",
    "    .filter(\"inventory_id is null\")\\\n",
    "    .select('title')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 5. Вивести топ 3 актори, які найбільше зʼявлялись в категорії фільмів “Children”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|actor_id|\n",
      "+--------+\n",
      "|      17|\n",
      "|      80|\n",
      "|     140|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "film_actor = spark.read.options(header='True', inferSchema='True').csv(\"data/film_actor.csv\")\n",
    "film_category = spark.read.options(header='True', inferSchema='True').csv(\"data/film_category.csv\")\n",
    "categories = spark.read.options(header='True', inferSchema='True').csv(\"data/category.csv\")\n",
    "\n",
    "child_cat = film_actor.join(film_category, on='film_id')\\\n",
    "    .join(categories, on='category_id')\\\n",
    "    .filter(\"name == 'Children'\")\\\n",
    "    .groupby('actor_id')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .limit(3)\\\n",
    "    .select('actor_id').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6. Вивести міста з кількістю активних та неактивних клієнтів (в активних customer.active = 1). Результат відсортувати за кількістю неактивних клієнтів за спаданням."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|city_id|active|inactive|\n",
      "+-------+------+--------+\n",
      "|    111|     0|       1|\n",
      "|    241|     1|       1|\n",
      "|    259|     0|       1|\n",
      "|    125|     0|       1|\n",
      "|     24|     0|       1|\n",
      "|    407|     0|       1|\n",
      "|    495|     0|       1|\n",
      "|    512|     0|       1|\n",
      "|    554|     0|       1|\n",
      "|    577|     0|       1|\n",
      "|    578|     0|       1|\n",
      "|    281|     0|       1|\n",
      "|    283|     0|       1|\n",
      "|     57|     0|       1|\n",
      "|    356|     0|       1|\n",
      "|    139|     0|       1|\n",
      "|      1|     1|       0|\n",
      "|      2|     1|       0|\n",
      "|      3|     1|       0|\n",
      "|      4|     1|       0|\n",
      "+-------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer = spark.read.options(header='True', inferSchema='True').csv(\"data/customer.csv\")\n",
    "address = spark.read.options(header='True', inferSchema='True').csv(\"data/address.csv\")\n",
    "city = spark.read.options(header='True', inferSchema='True').csv(\"data/city.csv\")\n",
    "\n",
    "customer_city_list = customer.join(address, on='address_id')\\\n",
    "    .join(city, on='city_id')\\\n",
    "    .select(['city_id', 'active'])\n",
    "\n",
    "city_active = customer_city_list.filter(\"active == 1\")\\\n",
    "    .groupby('city_id')\\\n",
    "    .count()\\\n",
    "    .withColumnRenamed(\"count\", \"active\")\n",
    "\n",
    "city_inactive = customer_city_list.filter(\"active == 0\")\\\n",
    "    .groupby('city_id')\\\n",
    "    .count()\\\n",
    "    .withColumnRenamed(\"count\", \"inactive\")\n",
    "\n",
    "city_active.join(city_inactive, on='city_id', how='full').na.fill(0).sort('inactive', ascending=False).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.14 64-bit ('3.7.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "88fcc5566fb1e594f903b658c85d165e2b9eb453a142f9879759ef78730ff8b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
